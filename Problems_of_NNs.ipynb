{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"10\"> Problems And Issues In Neural Networks </font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way.\n",
    "#### Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights  virtually unchanged, and training never converges to a good solution. This is called the **VANISHING GRADIENTS** problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the **EXPLODING GRADIENTS** problem, which is mostly encountered in recurrent neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To solve the above problem we need the **variance of the outputs of each layer to be equal to the variance of its inputs** and we also need the gradients to have **equal variance before and after flowing through a layer in the reverse direction**. It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the **fan-in and fan-out** of the layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To over come this we use **Xavier initialization** or **He initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x21c5b2d9610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### He initialization with a uniform distribution, but based on $ fan_{avg} $ rather than $ fan_{in} $, we can use the VarianceScaling initializer like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x21c10fd2e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                        distribution='uniform')\n",
    "\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ReLU activation function suffers from a problem known as the **dying ReLUs**: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate.\n",
    "\n",
    "#### To solve this problem, you may want to use a variant of the ReLU function, such as the **leaky ReLU** or **randomized leaky ReLU (RReLU)** or **parametric leaky ReLU (PReLU)**. Sole difference in the 3 being difference in α values. Also u can use **exponential linear unit (ELU)** and **Scaled ELU (SELU)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets train Fashion MNIST on leaky ReLU and PReLU and also SELU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 5ms/step - loss: 1.2641 - accuracy: 0.6143 - val_loss: 0.8490 - val_accuracy: 0.7210\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.7702 - accuracy: 0.7413 - val_loss: 0.6926 - val_accuracy: 0.7738\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.6655 - accuracy: 0.7796 - val_loss: 0.6335 - val_accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.6098 - accuracy: 0.7992 - val_loss: 0.5810 - val_accuracy: 0.8130\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5736 - accuracy: 0.8115 - val_loss: 0.5516 - val_accuracy: 0.8220\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5470 - accuracy: 0.8181 - val_loss: 0.5293 - val_accuracy: 0.8274\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5269 - accuracy: 0.8232 - val_loss: 0.5111 - val_accuracy: 0.8362\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5113 - accuracy: 0.8285 - val_loss: 0.5048 - val_accuracy: 0.8328\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4988 - accuracy: 0.8311 - val_loss: 0.4873 - val_accuracy: 0.8394\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4880 - accuracy: 0.8347 - val_loss: 0.4806 - val_accuracy: 0.8414\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 1.3628 - accuracy: 0.5709 - val_loss: 0.9141 - val_accuracy: 0.7040\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.8204 - accuracy: 0.7324 - val_loss: 0.7274 - val_accuracy: 0.7660\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.6985 - accuracy: 0.7739 - val_loss: 0.6547 - val_accuracy: 0.7932\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.6340 - accuracy: 0.7933 - val_loss: 0.5974 - val_accuracy: 0.8120\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5918 - accuracy: 0.8061 - val_loss: 0.5623 - val_accuracy: 0.8236\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5614 - accuracy: 0.8136 - val_loss: 0.5373 - val_accuracy: 0.8284\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5385 - accuracy: 0.8201 - val_loss: 0.5162 - val_accuracy: 0.8372\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5206 - accuracy: 0.8251 - val_loss: 0.5074 - val_accuracy: 0.8346\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5061 - accuracy: 0.8289 - val_loss: 0.4888 - val_accuracy: 0.8420\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4935 - accuracy: 0.8319 - val_loss: 0.4795 - val_accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lazy ReLU ==> 8s 5ms/step - loss: 0.4880 - accuracy: 0.8347 - val_loss: 0.4806 - val_accuracy: 0.8414\n",
    "####   PReLU   ==> 10s 6ms/step - loss: 0.4935 - accuracy: 0.8319 - val_loss: 0.4795 - val_accuracy: 0.8400\n",
    "\n",
    "#### All though we cannot see a difference except for the time taken but in complex conditions they do affect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                            kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Inputs for SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 69s 38ms/step - loss: 1.3340 - accuracy: 0.4734 - val_loss: 0.9590 - val_accuracy: 0.6292\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 67s 39ms/step - loss: 0.9277 - accuracy: 0.6541 - val_loss: 0.8934 - val_accuracy: 0.6664\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 67s 39ms/step - loss: 0.7940 - accuracy: 0.7074 - val_loss: 0.7187 - val_accuracy: 0.7358\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 62s 36ms/step - loss: 0.8667 - accuracy: 0.6788 - val_loss: 0.9225 - val_accuracy: 0.6914\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 65s 38ms/step - loss: 0.8008 - accuracy: 0.7124 - val_loss: 0.7559 - val_accuracy: 0.7374\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets try using ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 70s 39ms/step - loss: 1.8299 - accuracy: 0.2372 - val_loss: 1.6775 - val_accuracy: 0.2398\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 63s 36ms/step - loss: 1.2152 - accuracy: 0.4655 - val_loss: 0.9297 - val_accuracy: 0.6156\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 63s 37ms/step - loss: 0.9313 - accuracy: 0.6097 - val_loss: 0.9417 - val_accuracy: 0.6004\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 60s 35ms/step - loss: 0.8036 - accuracy: 0.6684 - val_loss: 0.7295 - val_accuracy: 0.7086\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 62s 36ms/step - loss: 0.8346 - accuracy: 0.6641 - val_loss: 0.7133 - val_accuracy: 0.7140\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Much worse accuracy, we suffered from the vanishing/exploding gradients problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
    "#### To ensure the the problems do not come back during training we have to use **Batch Normalization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape = [28, 28]),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(300, activation = 'elu', kernel_initializer = 'he_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(100, activation = \"elu\", kernel_initializer = 'he_normal'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(10, activation = 'softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 784)              3136      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_211 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= \"sparse_categorical_crossentropy\",\n",
    "                optimizer = keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 17s 9ms/step - loss: 0.8829 - accuracy: 0.7025 - val_loss: 0.5841 - val_accuracy: 0.8018\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.5951 - accuracy: 0.7939 - val_loss: 0.5076 - val_accuracy: 0.8320\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5427 - accuracy: 0.8103 - val_loss: 0.4722 - val_accuracy: 0.8436\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5075 - accuracy: 0.8237 - val_loss: 0.4507 - val_accuracy: 0.8512\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4867 - accuracy: 0.8297 - val_loss: 0.4355 - val_accuracy: 0.8560\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4711 - accuracy: 0.8349 - val_loss: 0.4237 - val_accuracy: 0.8592\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4595 - accuracy: 0.8374 - val_loss: 0.4126 - val_accuracy: 0.8604\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.4486 - accuracy: 0.8414 - val_loss: 0.4071 - val_accuracy: 0.8632\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.4390 - accuracy: 0.8450 - val_loss: 0.4000 - val_accuracy: 0.8634\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.4307 - accuracy: 0.8475 - val_loss: 0.3943 - val_accuracy: 0.8652\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another technique to mitigate the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold. This is called **Gradient Clipping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue = 1.0)\n",
    "model.compile(loss = 'mse', optimizer=optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)\n",
    "model.compile(loss = 'mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
