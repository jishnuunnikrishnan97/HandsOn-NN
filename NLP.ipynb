{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"10\"> ðŸ”¥NLPðŸ”¥ </font></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating an Shakespearean Text using Charecter RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the dataset and map them into all different charecter ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) \n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "31368/31368 [==============================] - 2708s 86ms/step - loss: 1.6186\n",
      "Epoch 2/2\n",
      "31368/31368 [==============================] - 2702s 86ms/step - loss: 1.5391\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
    "        keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "        ])\n",
    "tf.config.run_functions_eagerly(True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                    dropout=0.2, batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 9s 16ms/step - loss: 2.6195\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 2.2330\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 2.1041\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 2.0303\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.9804\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.9434\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.9173\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8952\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8776\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8608\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8494\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8384\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8294\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8192\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8117\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.8063\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7970\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7925\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7878\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7840\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7796\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7748\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7693\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7664\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7634\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7585\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7582\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7539\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7523\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7480\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7441\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7438\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7405\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7405\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7354\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7356\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7306\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7332\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7293\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7262\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7263\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7255\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7233\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.7213\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7208\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7163\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7189\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7141\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7171\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.7137\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=1000, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juliet:\n",
      "why doth not our saint joy with beautor worst.\n",
      "\n",
      "gloucester:\n",
      "ay 't so?\n",
      "\n",
      "lady cront york:\n",
      "sir, yourspect men for well marchuely with the\n",
      "intruck-haperal now wele peaced slaw sas our tongue.\n",
      "\n",
      "escalis:\n",
      "tas's so; for we hidies as soon broid east\n",
      "in pomson. the maids be will fondon him requition.\n",
      "\n",
      "clarence:\n",
      "now, sir,\n",
      "think, what nears, at, when his common soul,\n",
      "ground have the lord friends are grief; and, will great so lies to hight\n",
      "and he rest ant sweet a deters\n",
      "be fanosfal, to be equie was laading found, till to-your'd, have me dested:\n",
      "he will to this, as jonn tears on the samvers that pring's gone\n",
      "un't hids like, let humble lady of friends,\n",
      "somes' friends undid it whom it be so\n",
      "in spirent be shave him duke, if good of son:\n",
      "and wede none to lamentable ment. we that i love woman,\n",
      "where you shall be face me wost for thou to be painted.\n",
      "\n",
      "sampson:\n",
      "he would have turn, show, which have put apprince,\n",
      "which up so dear he chadge, we silter a\n",
      "constsain with this: love curtence paity dog\n",
      "into stirt\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"juliet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
